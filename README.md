Sign Language Translator
This project is a Sign Language Translator that allows real-time translation of sign language gestures into text using computer vision and machine learning techniques. It uses a webcam to capture gestures and translates them into corresponding text and speech. The application serves as a tool for the deaf and hard-of-hearing community, aiming to bridge communication gaps.

Features
Real-time Sign Language Translation: Capture sign language gestures via webcam and convert them into text.
Text-to-Speech Output: The translated text is spoken out loud for better accessibility.
Flask Backend: A RESTful API built with Flask for handling sign language image predictions and communication between the frontend and backend.
React Frontend: The frontend is built with React to create an interactive and user-friendly interface for sign language translation.
Model Integration: Utilizes pre-trained machine learning models for image recognition and prediction.
Tech Stack
Frontend: React, Vite, Webcam
Backend: Flask, OpenCV, TensorFlow (for prediction)
Machine Learning: Custom trained models for sign language recognition
Web API: REST API powered by Flask
Text-to-Speech: Integrated to provide audio feedback for translated text.
